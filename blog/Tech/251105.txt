3:I[4707,[],""]
6:I[6423,[],""]
7:I[4888,["972","static/chunks/972-d5884f09dde0387b.js","185","static/chunks/app/layout-9c3ad277717235be.js"],"GoogleAnalytics"]
8:I[2645,["972","static/chunks/972-d5884f09dde0387b.js","185","static/chunks/app/layout-9c3ad277717235be.js"],"Navbar"]
4:["category","Tech","d"]
5:["slug","251105","d"]
0:["bB_dy8pqBnRs1hZv6AeEb",[[["",{"children":["blog",{"children":[["category","Tech","d"],{"children":[["slug","251105","d"],{"children":["__PAGE__?{\"category\":\"Tech\",\"slug\":\"251105\"}",{}]}]}]}]},"$undefined","$undefined",true],["",{"children":["blog",{"children":[["category","Tech","d"],{"children":[["slug","251105","d"],{"children":["__PAGE__",{},[["$L1",["$","section",null,{"children":[["$","h1",null,{"className":"title font-semibold text-2xl tracking-tighter","children":"DIY GPT"}],["$","div",null,{"className":"flex justify-between items-center mt-2 mb-8 text-sm","children":["$","p",null,{"className":"text-sm text-neutral-600","children":"Nov 5, 2025"}]}],["$","article",null,{"className":"prose","children":"$L2"}]]}],[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/587fc2319eebab18.css","precedence":"next","crossOrigin":"$undefined"}]]],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$4","children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/4d66c1a699081f75.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","className":"text-black bg-white __variable_ac79ff __variable_8a4d12","children":[["$","meta",null,{"name":"msvalidate.01","content":"D1474E050BE527719E98F0704865EEAE"}],["$","$L7",null,{"gaId":"G-N3J3E2ME0X"}],["$","body",null,{"className":"antialiased max-w-4xl mx-4 mt-8 lg:mx-auto","children":["$","main",null,{"className":"flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0","children":[["$","$L8",null,{}],["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","section",null,{"children":[["$","h1",null,{"className":"mb-8 text-2xl font-semibold tracking-tighter","children":"404 - Page Not Found"}],["$","p",null,{"className":"mb-4","children":"The page you are looking for does not exist."}]]}],"notFoundStyles":[]}],["$","footer",null,{"className":"mb-16","children":[["$","ul",null,{"className":"font-sm mt-8 flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0","children":[["$","li",null,{"children":["$undefined",["$","p",null,{"className":" h-7 flex items-center transition-all hover:text-neutral-800","children":"United States"}]]}],["$","li",null,{"children":["$undefined",["$","p",null,{"className":" h-7 flex items-center transition-all hover:text-neutral-800","children":"Phone: 541-360-9231"}]]}]]}],["$","ul",null,{"className":"font-sm flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0","children":[["$","li",null,{"children":["$","a",null,{"className":"flex items-center transition-all hover:text-neutral-800","rel":"noopener noreferrer","target":"_blank","href":"https://github.com/bachsofttrick/","children":[["$","svg",null,{"width":"12","height":"12","viewBox":"0 0 12 12","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":["$","path",null,{"d":"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z","fill":"currentColor"}]}],["$","p",null,{"className":"ml-2 h-7","children":"github"}]]}]}],["$","li",null,{"children":["$","a",null,{"className":"flex items-center transition-all hover:text-neutral-800","rel":"noopener noreferrer","target":"_blank","href":"https://linkedin.com/in/brphan/","children":[["$","svg",null,{"width":"12","height":"12","viewBox":"0 0 12 12","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":["$","path",null,{"d":"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z","fill":"currentColor"}]}],["$","p",null,{"className":"ml-2 h-7","children":"linkedin"}]]}]}],["$","li",null,{"children":["$","a",null,{"className":"flex items-center transition-all hover:text-neutral-800","rel":"noopener noreferrer","target":"_blank","href":"mailto:xuanbach1307@gmail.com","children":[["$","svg",null,{"width":"12","height":"12","viewBox":"0 0 12 12","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":["$","path",null,{"d":"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z","fill":"currentColor"}]}],["$","p",null,{"className":"ml-2 h-7","children":"email"}]]}]}],["$","li",null,{"children":["$","a",null,{"className":"flex items-center transition-all hover:text-neutral-800","rel":"noopener noreferrer","target":"_blank","href":"https://github.com/bachsofttrick/bachsofttrick.github.io/raw/refs/heads/main/resume.docx","children":[["$","svg",null,{"width":"12","height":"12","viewBox":"0 0 12 12","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":["$","path",null,{"d":"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z","fill":"currentColor"}]}],["$","p",null,{"className":"ml-2 h-7","children":"get resume"}]]}]}],["$","li",null,{"children":["$","a",null,{"className":"flex items-center transition-all hover:text-neutral-800","rel":"noopener noreferrer","target":"_blank","href":"/rss.xml","children":[["$","svg",null,{"width":"12","height":"12","viewBox":"0 0 12 12","fill":"none","xmlns":"http://www.w3.org/2000/svg","children":["$","path",null,{"d":"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z","fill":"currentColor"}]}],["$","p",null,{"className":"ml-2 h-7","children":"rss"}]]}]}]]}],["$","p",null,{"className":"mt-8 text-neutral-600","children":2026}]]}]]}]}]]}]],null],null],["$L9",null]]]]
2:[["$","p",null,{"children":"I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has\nbeen so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT models, only its uses\nand effectiveness. Despite its overtly cherry nature, ChatGPT 4 and 4o represented the best OpenAI has to offer\nin terms of speed and accuracy. Those reasoning models have never been appealing to me. They are just models reiterating their\nown answers over and over again. These are \"souped up\" chatbots, no reasoning capability. And sometimes I just\nneed a quick answer. Right or wrong, ChatGPT always gives me a starting point. They make great rubber ducks to iterate\nideas with. Even since OpenAI \"upgrades\" to version 5, ChatGPT has become sluggish. It is not more correct, nor faster\nthan previous models. They even disabled your ability to switch models. Like bro, I don't need a chatbot to spend\na minute coming up with an answer. Just give me something to start."}],"\n",["$","p",null,{"children":["I looked at some ways to run LLMs locally before. But I was turned off by how much VRAM they need to run big models\nlike gpt-oss. And then, something randomly clicked yesterday. Pretty sure I just searched something randomly, as\nalways. And I came across ",["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://ollama.com/library","children":"ollama"}],". There are models that are much smaller than the\nbig boy gpt-oss. Like the deepseek-r1:8b, or the gemma3:4b. The 8b stands for billion parameters. The bigger the number,\nthe more accurate the output, but also the larger the size. So these two models fit perfectly in my 6GB GTX 1660 Ti.\nAnd the ollama is easy to install and use. I chose the manual installation method: download an archive and run its binary.\nIts interface is reminiscent of Docker. So just ",["$","code",null,{"dangerouslySetInnerHTML":{"__html":"<span class=\"sh__line\"><span class=\"sh__token--identifier\" style=\"color: var(--sh-identifier)\">ollama</span><span class=\"sh__token--space\" style=\"color: var(--sh-space)\"> </span><span class=\"sh__token--identifier\" style=\"color: var(--sh-identifier)\">serve</span><span class=\"sh__token--space\" style=\"color: var(--sh-space)\"> </span><span class=\"sh__token--sign\" style=\"color: var(--sh-sign)\">&amp;</span></span>"}}]," first to create a server. Then ",["$","code",null,{"dangerouslySetInnerHTML":{"__html":"<span class=\"sh__line\"><span class=\"sh__token--identifier\" style=\"color: var(--sh-identifier)\">ollama</span><span class=\"sh__token--space\" style=\"color: var(--sh-space)\"> </span><span class=\"sh__token--identifier\" style=\"color: var(--sh-identifier)\">pull</span><span class=\"sh__token--space\" style=\"color: var(--sh-space)\"> </span><span class=\"sh__token--sign\" style=\"color: var(--sh-sign)\">&lt;</span><span class=\"sh__token--entity\" style=\"color: var(--sh-entity)\">model</span><span class=\"sh__token--sign\" style=\"color: var(--sh-sign)\">&gt;</span></span>"}}],"\nand ",["$","code",null,{"dangerouslySetInnerHTML":{"__html":"<span class=\"sh__line\"><span class=\"sh__token--identifier\" style=\"color: var(--sh-identifier)\">ollama</span><span class=\"sh__token--space\" style=\"color: var(--sh-space)\"> </span><span class=\"sh__token--identifier\" style=\"color: var(--sh-identifier)\">run</span><span class=\"sh__token--space\" style=\"color: var(--sh-space)\"> </span><span class=\"sh__token--sign\" style=\"color: var(--sh-sign)\">&lt;</span><span class=\"sh__token--entity\" style=\"color: var(--sh-entity)\">model</span><span class=\"sh__token--sign\" style=\"color: var(--sh-sign)\">&gt;</span></span>"}}]," and you are good."]}],"\n",["$","p",null,{"children":["I read some online posts on Reddit, ",["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://old.reddit.com/r/LocalLLaMA/","children":"r/LocalLLaMA"}],". There seems to be an all-out war\nbetween these three applications:"]}],"\n",["$","ul",null,{"children":["\n",["$","li",null,{"children":["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://github.com/ggml-org/llama.cpp","children":"llama.cpp"}]}],"\n",["$","li",null,{"children":["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://ollama.com/","children":"ollama"}]}],"\n",["$","li",null,{"children":["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://lmstudio.ai/","children":"LM studio"}]}],"\n"]}],"\n",["$","p",null,{"children":["I won't go into details, because even I am at a loss myself. I use ollama because it's easy to get your feet wet, like Docker\nbefore learning Kubernetes. ollama loses out to llama.cpp in terms of performance, functions. There are\n",["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://github.com/ollama/ollama/issues/11714#issuecomment-3172893576","children":"beefs"}]," between the two applications.\nIf I ever decided to get serious about this running local GPT seriously, I will consider llama.cpp. Or vllm like\n",["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/","children":"this guy"}],"."]}],"\n",["$","p",null,{"children":"Other than that, I am just happy to have my own DIY GPT."}],"\n",["$","br",null,{}],"\n",["$","span",null,{"children":[["$","span",null,{"className":"italic","children":"‚ÄùThis content was not written, nor helped written, by ChatGPT, Gemini, DeepSeek, or any of the LLMs. Viewer discretion is advised.\""}]," "]}]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"DIY GPT | Brian Phan"}],["$","meta","3",{"name":"description","content":"I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has\nbeen so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ..."}],["$","meta","4",{"name":"robots","content":"index, follow"}],["$","meta","5",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","meta","6",{"property":"og:title","content":"DIY GPT"}],["$","meta","7",{"property":"og:description","content":"I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has\nbeen so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ..."}],["$","meta","8",{"property":"og:url","content":"https://bachsofttrick.github.io/blog/Tech/251105"}],["$","meta","9",{"property":"og:type","content":"article"}],["$","meta","10",{"property":"article:published_time","content":"2025-11-05"}],["$","meta","11",{"property":"article:section","content":"Tech"}],["$","meta","12",{"name":"twitter:card","content":"summary"}],["$","meta","13",{"name":"twitter:title","content":"DIY GPT"}],["$","meta","14",{"name":"twitter:description","content":"I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has\nbeen so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ..."}],["$","meta","15",{"name":"next-size-adjust"}]]
1:null
