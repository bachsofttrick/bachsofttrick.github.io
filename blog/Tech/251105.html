<!DOCTYPE html><html lang="en" class="text-black bg-white __variable_ac79ff __variable_8a4d12"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/ec1a1eae803b668e-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/f980ec13b5b5e554.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/4d66c1a699081f75.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/587fc2319eebab18.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-904fb5a61c4981e8.js"/><script src="/_next/static/chunks/fd9d1056-ec0959f0939cb609.js" async=""></script><script src="/_next/static/chunks/117-e95138054262183e.js" async=""></script><script src="/_next/static/chunks/main-app-b4a7286994ec4cca.js" async=""></script><script src="/_next/static/chunks/972-d5884f09dde0387b.js" async=""></script><script src="/_next/static/chunks/app/layout-9c3ad277717235be.js" async=""></script><link rel="preload" href="https://www.googletagmanager.com/gtag/js?id=G-N3J3E2ME0X" as="script"/><title>DIY GPT | Brian Phan</title><meta name="description" content="I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has
been so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ..."/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="DIY GPT"/><meta property="og:description" content="I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has
been so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ..."/><meta property="og:url" content="https://bachsofttrick.github.io/blog/Tech/251105"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2025-11-05"/><meta property="article:section" content="Tech"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="DIY GPT"/><meta name="twitter:description" content="I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has
been so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ..."/><meta name="next-size-adjust"/><meta name="msvalidate.01" content="D1474E050BE527719E98F0704865EEAE"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased max-w-4xl mx-4 mt-8 lg:mx-auto"><main class="flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0"><aside class="-ml-[8px] mb-16 tracking-tight"><div class="lg:sticky lg:top-20"><nav class="flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative" id="nav"><div class="flex flex-row space-x-0 pr-10"><a class="transition-all hover:text-neutral-800 flex align-middle relative py-1 px-2 m-1" href="/">home</a><a class="transition-all hover:text-neutral-800 flex align-middle relative py-1 px-2 m-1" href="/about">about</a><a class="transition-all hover:text-neutral-800 flex align-middle relative py-1 px-2 m-1" href="/projects">projects</a><a class="transition-all hover:text-neutral-800 flex align-middle relative py-1 px-2 m-1" href="/blog">blog</a><a class="transition-all hover:text-neutral-800 flex align-middle relative py-1 px-2 m-1" href="">▷</a><audio id="audio" preload="auto" src="/music/Giornos Theme, but only the best part.m4a"></audio></div></nav></div></aside><section><h1 class="title font-semibold text-2xl tracking-tighter">DIY GPT</h1><div class="flex justify-between items-center mt-2 mb-8 text-sm"><p class="text-sm text-neutral-600">Nov 5, 2025</p></div><article class="prose"><p>I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has
been so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT models, only its uses
and effectiveness. Despite its overtly cherry nature, ChatGPT 4 and 4o represented the best OpenAI has to offer
in terms of speed and accuracy. Those reasoning models have never been appealing to me. They are just models reiterating their
own answers over and over again. These are &quot;souped up&quot; chatbots, no reasoning capability. And sometimes I just
need a quick answer. Right or wrong, ChatGPT always gives me a starting point. They make great rubber ducks to iterate
ideas with. Even since OpenAI &quot;upgrades&quot; to version 5, ChatGPT has become sluggish. It is not more correct, nor faster
than previous models. They even disabled your ability to switch models. Like bro, I don&#x27;t need a chatbot to spend
a minute coming up with an answer. Just give me something to start.</p>
<p>I looked at some ways to run LLMs locally before. But I was turned off by how much VRAM they need to run big models
like gpt-oss. And then, something randomly clicked yesterday. Pretty sure I just searched something randomly, as
always. And I came across <a target="_blank" rel="noopener noreferrer" href="https://ollama.com/library">ollama</a>. There are models that are much smaller than the
big boy gpt-oss. Like the deepseek-r1:8b, or the gemma3:4b. The 8b stands for billion parameters. The bigger the number,
the more accurate the output, but also the larger the size. So these two models fit perfectly in my 6GB GTX 1660 Ti.
And the ollama is easy to install and use. I chose the manual installation method: download an archive and run its binary.
Its interface is reminiscent of Docker. So just <code><span class="sh__line"><span class="sh__token--identifier" style="color: var(--sh-identifier)">ollama</span><span class="sh__token--space" style="color: var(--sh-space)"> </span><span class="sh__token--identifier" style="color: var(--sh-identifier)">serve</span><span class="sh__token--space" style="color: var(--sh-space)"> </span><span class="sh__token--sign" style="color: var(--sh-sign)">&amp;</span></span></code> first to create a server. Then <code><span class="sh__line"><span class="sh__token--identifier" style="color: var(--sh-identifier)">ollama</span><span class="sh__token--space" style="color: var(--sh-space)"> </span><span class="sh__token--identifier" style="color: var(--sh-identifier)">pull</span><span class="sh__token--space" style="color: var(--sh-space)"> </span><span class="sh__token--sign" style="color: var(--sh-sign)">&lt;</span><span class="sh__token--entity" style="color: var(--sh-entity)">model</span><span class="sh__token--sign" style="color: var(--sh-sign)">&gt;</span></span></code>
and <code><span class="sh__line"><span class="sh__token--identifier" style="color: var(--sh-identifier)">ollama</span><span class="sh__token--space" style="color: var(--sh-space)"> </span><span class="sh__token--identifier" style="color: var(--sh-identifier)">run</span><span class="sh__token--space" style="color: var(--sh-space)"> </span><span class="sh__token--sign" style="color: var(--sh-sign)">&lt;</span><span class="sh__token--entity" style="color: var(--sh-entity)">model</span><span class="sh__token--sign" style="color: var(--sh-sign)">&gt;</span></span></code> and you are good.</p>
<p>I read some online posts on Reddit, <a target="_blank" rel="noopener noreferrer" href="https://old.reddit.com/r/LocalLLaMA/">r/LocalLLaMA</a>. There seems to be an all-out war
between these three applications:</p>
<ul>
<li><a target="_blank" rel="noopener noreferrer" href="https://github.com/ggml-org/llama.cpp">llama.cpp</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="https://ollama.com/">ollama</a></li>
<li><a target="_blank" rel="noopener noreferrer" href="https://lmstudio.ai/">LM studio</a></li>
</ul>
<p>I won&#x27;t go into details, because even I am at a loss myself. I use ollama because it&#x27;s easy to get your feet wet, like Docker
before learning Kubernetes. ollama loses out to llama.cpp in terms of performance, functions. There are
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ollama/ollama/issues/11714#issuecomment-3172893576">beefs</a> between the two applications.
If I ever decided to get serious about this running local GPT seriously, I will consider llama.cpp. Or vllm like
<a target="_blank" rel="noopener noreferrer" href="https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/">this guy</a>.</p>
<p>Other than that, I am just happy to have my own DIY GPT.</p>
<br/>
<span><span class="italic">”This content was not written, nor helped written, by ChatGPT, Gemini, DeepSeek, or any of the LLMs. Viewer discretion is advised.&quot;</span> </span></article></section><footer class="mb-16"><ul class="font-sm mt-8 flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0"><li><p class=" h-7 flex items-center transition-all hover:text-neutral-800">United States</p></li><li><p class=" h-7 flex items-center transition-all hover:text-neutral-800">Phone: 541-360-9231</p></li></ul><ul class="font-sm flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0"><li><a class="flex items-center transition-all hover:text-neutral-800" rel="noopener noreferrer" target="_blank" href="https://github.com/bachsofttrick/"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">github</p></a></li><li><a class="flex items-center transition-all hover:text-neutral-800" rel="noopener noreferrer" target="_blank" href="https://linkedin.com/in/brphan/"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">linkedin</p></a></li><li><a class="flex items-center transition-all hover:text-neutral-800" rel="noopener noreferrer" target="_blank" href="mailto:xuanbach1307@gmail.com"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">email</p></a></li><li><a class="flex items-center transition-all hover:text-neutral-800" rel="noopener noreferrer" target="_blank" href="https://github.com/bachsofttrick/bachsofttrick.github.io/raw/refs/heads/main/resume.docx"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">get resume</p></a></li><li><a class="flex items-center transition-all hover:text-neutral-800" rel="noopener noreferrer" target="_blank" href="/rss.xml"><svg width="12" height="12" viewBox="0 0 12 12" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z" fill="currentColor"></path></svg><p class="ml-2 h-7">rss</p></a></li></ul><p class="mt-8 text-neutral-600">2026</p></footer></main><script src="/_next/static/chunks/webpack-904fb5a61c4981e8.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/ec1a1eae803b668e-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/f980ec13b5b5e554.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/4d66c1a699081f75.css\",\"style\"]\n4:HL[\"/_next/static/css/587fc2319eebab18.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"5:I[2846,[],\"\"]\n8:I[4707,[],\"\"]\nb:I[6423,[],\"\"]\nc:I[4888,[\"972\",\"static/chunks/972-d5884f09dde0387b.js\",\"185\",\"static/chunks/app/layout-9c3ad277717235be.js\"],\"GoogleAnalytics\"]\nd:I[2645,[\"972\",\"static/chunks/972-d5884f09dde0387b.js\",\"185\",\"static/chunks/app/layout-9c3ad277717235be.js\"],\"Navbar\"]\nf:I[1060,[],\"\"]\n9:[\"category\",\"Tech\",\"d\"]\na:[\"slug\",\"251105\",\"d\"]\n10:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L5\",null,{\"buildId\":\"bB_dy8pqBnRs1hZv6AeEb\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"blog\",\"Tech\",\"251105\"],\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"category\",\"Tech\",\"d\"],{\"children\":[[\"slug\",\"251105\",\"d\"],{\"children\":[\"__PAGE__?{\\\"category\\\":\\\"Tech\\\",\\\"slug\\\":\\\"251105\\\"}\",{}]}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"category\",\"Tech\",\"d\"],{\"children\":[[\"slug\",\"251105\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L6\",[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"title font-semibold text-2xl tracking-tighter\",\"children\":\"DIY GPT\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center mt-2 mb-8 text-sm\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-sm text-neutral-600\",\"children\":\"Nov 5, 2025\"}]}],[\"$\",\"article\",null,{\"className\":\"prose\",\"children\":\"$L7\"}]]}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/587fc2319eebab18.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]],null],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$9\",\"children\",\"$a\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$9\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4d66c1a699081f75.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"text-black bg-white __variable_ac79ff __variable_8a4d12\",\"children\":[[\"$\",\"meta\",null,{\"name\":\"msvalidate.01\",\"content\":\"D1474E050BE527719E98F0704865EEAE\"}],[\"$\",\"$Lc\",null,{\"gaId\":\"G-N3J3E2ME0X\"}],[\"$\",\"body\",null,{\"className\":\"antialiased max-w-4xl mx-4 mt-8 lg:mx-auto\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0\",\"children\":[[\"$\",\"$Ld\",null,{}],[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Lb\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-8 text-2xl font-semibold tracking-tighter\",\"children\":\"404 - Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"The page you are looking for does not exist.\"}]]}],\"notFoundStyles\":[]}],[\"$\",\"footer\",null,{\"className\":\"mb-16\",\"children\":[[\"$\",\"ul\",null,{\"className\":\"font-sm mt-8 flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$undefined\",[\"$\",\"p\",null,{\"className\":\" h-7 flex items-center transition-all hover:text-neutral-800\",\"children\":\"United States\"}]]}],[\"$\",\"li\",null,{\"children\":[\"$undefined\",[\"$\",\"p\",null,{\"className\":\" h-7 flex items-center transition-all hover:text-neutral-800\",\"children\":\"Phone: 541-360-9231\"}]]}]]}],[\"$\",\"ul\",null,{\"className\":\"font-sm flex flex-col space-x-0 space-y-2 text-neutral-600 md:flex-row md:space-x-4 md:space-y-0\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://github.com/bachsofttrick/\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"github\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://linkedin.com/in/brphan/\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"linkedin\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"mailto:xuanbach1307@gmail.com\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"email\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"https://github.com/bachsofttrick/bachsofttrick.github.io/raw/refs/heads/main/resume.docx\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"get resume\"}]]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"className\":\"flex items-center transition-all hover:text-neutral-800\",\"rel\":\"noopener noreferrer\",\"target\":\"_blank\",\"href\":\"/rss.xml\",\"children\":[[\"$\",\"svg\",null,{\"width\":\"12\",\"height\":\"12\",\"viewBox\":\"0 0 12 12\",\"fill\":\"none\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"children\":[\"$\",\"path\",null,{\"d\":\"M2.07102 11.3494L0.963068 10.2415L9.2017 1.98864H2.83807L2.85227 0.454545H11.8438V9.46023H10.2955L10.3097 3.09659L2.07102 11.3494Z\",\"fill\":\"currentColor\"}]}],[\"$\",\"p\",null,{\"className\":\"ml-2 h-7\",\"children\":\"rss\"}]]}]}]]}],[\"$\",\"p\",null,{\"className\":\"mt-8 text-neutral-600\",\"children\":2026}]]}]]}]}]]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Le\"],\"globalErrorComponent\":\"$f\",\"missingSlots\":\"$W10\"}]\n"])</script><script>self.__next_f.push([1,"7:[[\"$\",\"p\",null,{\"children\":\"I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has\\nbeen so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT models, only its uses\\nand effectiveness. Despite its overtly cherry nature, ChatGPT 4 and 4o represented the best OpenAI has to offer\\nin terms of speed and accuracy. Those reasoning models have never been appealing to me. They are just models reiterating their\\nown answers over and over again. These are \\\"souped up\\\" chatbots, no reasoning capability. And sometimes I just\\nneed a quick answer. Right or wrong, ChatGPT always gives me a starting point. They make great rubber ducks to iterate\\nideas with. Even since OpenAI \\\"upgrades\\\" to version 5, ChatGPT has become sluggish. It is not more correct, nor faster\\nthan previous models. They even disabled your ability to switch models. Like bro, I don't need a chatbot to spend\\na minute coming up with an answer. Just give me something to start.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"I looked at some ways to run LLMs locally before. But I was turned off by how much VRAM they need to run big models\\nlike gpt-oss. And then, something randomly clicked yesterday. Pretty sure I just searched something randomly, as\\nalways. And I came across \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://ollama.com/library\",\"children\":\"ollama\"}],\". There are models that are much smaller than the\\nbig boy gpt-oss. Like the deepseek-r1:8b, or the gemma3:4b. The 8b stands for billion parameters. The bigger the number,\\nthe more accurate the output, but also the larger the size. So these two models fit perfectly in my 6GB GTX 1660 Ti.\\nAnd the ollama is easy to install and use. I chose the manual installation method: download an archive and run its binary.\\nIts interface is reminiscent of Docker. So just \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"sh__line\\\"\u003e\u003cspan class=\\\"sh__token--identifier\\\" style=\\\"color: var(--sh-identifier)\\\"\u003eollama\u003c/span\u003e\u003cspan class=\\\"sh__token--space\\\" style=\\\"color: var(--sh-space)\\\"\u003e \u003c/span\u003e\u003cspan class=\\\"sh__token--identifier\\\" style=\\\"color: var(--sh-identifier)\\\"\u003eserve\u003c/span\u003e\u003cspan class=\\\"sh__token--space\\\" style=\\\"color: var(--sh-space)\\\"\u003e \u003c/span\u003e\u003cspan class=\\\"sh__token--sign\\\" style=\\\"color: var(--sh-sign)\\\"\u003e\u0026amp;\u003c/span\u003e\u003c/span\u003e\"}}],\" first to create a server. Then \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"sh__line\\\"\u003e\u003cspan class=\\\"sh__token--identifier\\\" style=\\\"color: var(--sh-identifier)\\\"\u003eollama\u003c/span\u003e\u003cspan class=\\\"sh__token--space\\\" style=\\\"color: var(--sh-space)\\\"\u003e \u003c/span\u003e\u003cspan class=\\\"sh__token--identifier\\\" style=\\\"color: var(--sh-identifier)\\\"\u003epull\u003c/span\u003e\u003cspan class=\\\"sh__token--space\\\" style=\\\"color: var(--sh-space)\\\"\u003e \u003c/span\u003e\u003cspan class=\\\"sh__token--sign\\\" style=\\\"color: var(--sh-sign)\\\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\\\"sh__token--entity\\\" style=\\\"color: var(--sh-entity)\\\"\u003emodel\u003c/span\u003e\u003cspan class=\\\"sh__token--sign\\\" style=\\\"color: var(--sh-sign)\\\"\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\"}}],\"\\nand \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\u003cspan class=\\\"sh__line\\\"\u003e\u003cspan class=\\\"sh__token--identifier\\\" style=\\\"color: var(--sh-identifier)\\\"\u003eollama\u003c/span\u003e\u003cspan class=\\\"sh__token--space\\\" style=\\\"color: var(--sh-space)\\\"\u003e \u003c/span\u003e\u003cspan class=\\\"sh__token--identifier\\\" style=\\\"color: var(--sh-identifier)\\\"\u003erun\u003c/span\u003e\u003cspan class=\\\"sh__token--space\\\" style=\\\"color: var(--sh-space)\\\"\u003e \u003c/span\u003e\u003cspan class=\\\"sh__token--sign\\\" style=\\\"color: var(--sh-sign)\\\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\\\"sh__token--entity\\\" style=\\\"color: var(--sh-entity)\\\"\u003emodel\u003c/span\u003e\u003cspan class=\\\"sh__token--sign\\\" style=\\\"color: var(--sh-sign)\\\"\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\"}}],\" and you are good.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"I read some online posts on Reddit, \",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://old.reddit.com/r/LocalLLaMA/\",\"children\":\"r/LocalLLaMA\"}],\". There seems to be an all-out war\\nbetween these three applications:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/ggml-org/llama.cpp\",\"children\":\"llama.cpp\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://ollama.com/\",\"children\":\"ollama\"}]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://lmstudio.ai/\",\"children\":\"LM studio\"}]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"I won't go into details, because even I am at a loss myself. I use ollama because it's easy to get your feet wet, like Docker\\nbefore learning Kubernetes. ollama loses out to llama.cpp in terms of performance, functions. There are\\n\",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://github.com/ollama/ollama/issues/11714#issuecomment-3172893576\",\"children\":\"beefs\"}],\" between the two applications.\\nIf I ever decided to get serious about this running local GPT seriously, I will consider llama.cpp. Or vllm like\\n\",[\"$\",\"a\",null,{\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"href\":\"https://old.reddit.com/r/LocalLLaMA/comments/1opa6os/local_setup/\",\"children\":\"this guy\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Other than that, I am just happy to have my own DIY GPT.\"}],\"\\n\",[\"$\",\"br\",null,{}],\"\\n\",[\"$\",\"span\",null,{\"children\":[[\"$\",\"span\",null,{\"className\":\"italic\",\"children\":\"”This content was not written, nor helped written, by ChatGPT, Gemini, DeepSeek, or any of the LLMs. Viewer discretion is advised.\\\"\"}],\" \"]}]]\n"])</script><script>self.__next_f.push([1,"e:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"DIY GPT | Brian Phan\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has\\nbeen so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ...\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"DIY GPT\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has\\nbeen so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ...\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://bachsofttrick.github.io/blog/Tech/251105\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"10\",{\"property\":\"article:published_time\",\"content\":\"2025-11-05\"}],[\"$\",\"meta\",\"11\",{\"property\":\"article:section\",\"content\":\"Tech\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"DIY GPT\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"I have a dream. That I, one day, shall have my own ChatGPT. Because after being an avid user since 2023, it has\\nbeen so slow and laggy. Not like many people, I hold no emotional attachment to ChatGPT ...\"}],[\"$\",\"meta\",\"15\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script></body></html>