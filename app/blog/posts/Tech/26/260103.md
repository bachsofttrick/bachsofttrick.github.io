---
title: First project with llama-cpp-python
publishedAt: 2026-01-03
order: 1
hidden: true
---
llama-cpp-python is a Python wrapper for the llama.cpp library.
Call C functions directly, hence the need for compiling before installing, support for low-level access.

```
# Import mmproj for vision part of the multimodal
chat_handler = Llava15ChatHandler(
    clip_model_path='models/noctrex_LightOnOCR-1B-1025-GGUF_mmproj-F16.gguf'
)

llm = Llama(
    model_path='models/LightOnOCR-1B-1025-Q4_K_M.gguf',
    n_gpu_layers=99,
    n_ctx=0,
    chat_handler=chat_handler,
    verbose=False
)
```

error:
```
clip_init: failed to load model 'models/noctrex_LightOnOCR-1B-1025-GGUF_mmproj-F16.gguf': load_hparams: unknown projector type: lightonocr

mtmd_init_from_file: error: Failed to load CLIP model from models/noctrex_LightOnOCR-1B-1025-GGUF_mmproj-F16.gguf
```

Dig deep into the Python library, found this connection to the C library:
```
@ctypes_function(
    "mtmd_init_from_file",
    [c_char_p, llama_cpp.llama_model_p_ctypes, mtmd_context_params],
    mtmd_context_p_ctypes
)
```
Seems like it calls function mtmd_init_from_file() from "tools/mtmd/mtmd.cpp" accrding to the source code
of llama.cpp.

Run it normally using the newest version of llama.cpp, use LightOnOcr normally.
In "tools/mtmd/clip.cpp", I found clip_init() and load_hparams(). From there, located "lightonocr"
in "tools/mtmd/clip.impl.h". 
```
{ PROJECTOR_TYPE_LIGHTONOCR,"lightonocr"}
```
So why didn't it work on llama-cpp-python? Turns out llama-cpp-python used (this commit from llama.cpp)[https://github.com/ggerganov/llama.cpp/tree/4227c9be4268ac844921b90f31595f81236bd317], which is 5 months out-of-date. I guess they weren't kidding about which [models](https://llama-cpp-python.readthedocs.io/en/latest/#multi-modal-models) do they support for multi-modal.

Can't just copy shared libraries over. Cause conflicts like non existing functions:
```
AttributeError: /app/venv/lib/python3.12/site-packages/llama_cpp/lib/libllama.so: undefined symbol: llama_get_kv_self. Did you mean: 'llama_get_model'?
```
So either wait for new release, patch it in myself. I can just use OpenAI API provided by llama-server
to continue, or find another model that this library supports.