---
title: 'Getting Nvidia GPU to work in Docker container: A Cursed Experience'
publishedAt: '2025-11-09'
hidden: true
---
After using ollama to run local LLMs on my computer, I recently switched to llama.cpp. That was because ollama,
when running a model, would use up a lot of CPU power, despite the entire model being on 100% GPU. The whole
laptop sounded like a plane taking off anytime it tried to create anything coherent. [llama.cpp](https://github.com/ggml-org/llama.cpp) is faster (not using much CPU), use Vulkan as the backend for inference so cross compatible
with the Intel integrated GPU (although you can compile one with CUDA in mind).

I tested llama.cpp using various backend:

<table>
  <thead>
    <tr>
      <th>Backend</th>
      <th>Tokens/s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CPU only (i7-9750H)</td>
      <td className="text-center">5</td>
    </tr>
    <tr>
      <td>Intel UHD Graphics 630 (Vulkan)</td>
      <td className="text-center">2</td>
    </tr>
    <tr>
      <td>Nvidia Geforce GTX 1660 Ti (Vulkan)</td>
      <td className="text-center">31</td>
    </tr>
    <tr>
      <td>Nvidia Geforce GTX 1660 Ti (CUDA)</td>
      <td className="text-center">?</td>
    </tr>
  </tbody>
</table>



